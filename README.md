# WebSearchAI
A Local LLM Chat Tool with built in Web Searching

### The Skinny
Tired of using online AI Models to solve your problems, just so they can sell your data to the highest bidder?
Well so was I. So I built a brand new tool to chat with Local AI LLMs, and extend them with the power of **THE INTERNET**.

## Quick Start
1. Open a terminal where you want to install the project
2. `git clone https://github.com/Drinkingpants74/WebSearchAI`
3. Run `start.sh` or `install.sh` and follow the instructions.
4. Profit.

## Description
The goal of the `WebSearchAI` project is just as the name states; Give Local LLMs the ability to search the web.

Local LLMs are a really useful tool. The problem is they minute they'remade, the information inside them is out of date. So how do we fix that?
We let the AI search the web for modern information.

### Current Features
* Local Chat with LLM
* AI can "Search" to help answer your prompt
* Character Cards

### Requirements
* Python3
* A Supported Device
* GGUF Models

### Supported Device
* **_Nvidia w/ CUDA_**
* **_AMD w/ Vulkan_**
* **_Apple w/ Metal_**
* **_Intel w/ SYCL & OneAPI_**
* **_CPU_**

#### ROCm Support
As an AMD user myself, ROCm support is something I'd love to add. However, the `llama-cpp-python` library seems to struggle with ROCm.
That, or I'm just stupid. So for now, AMD cards will use Vulkan.

#### ‚ö†Ô∏èWARNING‚ö†Ô∏è
**This project uses Artificial Intelligence. Information given by AI models may not always be factual.**

#### üõëNOTICEüõë
**_PLEASE_ use this project responsibiy. I am not responsible for the actions of the user. By downloading and using this project, you (the user) assume all responsiblty.**
